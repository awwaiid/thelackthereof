---
title: Recurse - W1/6 D5/5
tags: ["blog","recurse-center"]
createdAt: 2024-03-29
updatedAt: 2024-03-29
---

Llamas, Pythons, Coffee, and Sway

## Part I - Morning

* **Morning Status:**
  * I feel pretty great today, but going to be Virtual anyway. Got some errands to run
  * I've got llama-cpp-python up and running, so I'm going to work on a back-end for it
  * I'll likely solicit pairing once I get it a bit further, especially for some visuals

## Part II - Evening

* **General Status:**
  * TIL: Drop this in your python code to get a pretty close `binding.pry` equivalent ```from IPython import embed # For debugging; put `embed()` anywhere```
  * Coffee Chat with Paolo!
  * Volitional Muscles Workshop helped me re-affirm that I like the plan that I pre-gamed
    * Core: Alloy+LLM+Rails (using Human Essentials app as a dataset)
      * Explore possibilities, mash together things, dev-tooling, do some Good
    * Cake: Pair, Follow Whims, Dance With Serendipity
      * Create artifacts, play, learn, share
  * Side Note: I'm typing this in vim with copilot and it is upsetting when it suggests stiff language and then it is upsetting when it suggests exactly what I was about to write because then I'm like ... is this as bad as what you just suggested dear robot?
  * Attended applied machine learning group, but really just watched
  * .... then got a Work Call that I hopped on to help with for the end of the day
* **Alloy+LLM+Rails Project:**
  * Slight progress on understanding the llama-cpp-python setup
  * Did some brushing up on python iterators and debugging tools

## Part III - WEEKEND BONUS

* **Weekend Updates:**
  * Nerd Sniped on displaying the current desktop grid pop-up to the right monitor. Got it working :)
    * For reference, `screen=$(swaymsg -t get_outputs | jq 'map(.focused) | index(true)')` which I then pass on to aosd_cat
  * I dreamed about generating tokens from an LLM, but I think they were physical tokens and also maybe gangsters
  * I don't really need it now, but I keep wondering a cost-effective but natural-for-me way to rent GPU time to make my model run faster. I can spin up a whole linode GPU vm, but will need to make sure to delete it in-between uses. I don't really want to do hosted notebook blah blah. Not sure what I want. I guess I wish I could run a command and have a GPU on my existing linode and then run another to remove it, but that's maybe not how it works
  * We went on a some shopping and walking around and out to Thai and got groceries. Making good progress on getting the sense-of-place around here
